{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "weXEeZ5mIeKF",
        "outputId": "1f184bd6-2eeb-4664-fe64-8d9bb12baaca"
      },
      "outputs": [],
      "source": [
        "pip install openai langchain langchain-community pinecone-client python-dotenv langchain-pinecone tiktoken sentence_transformers chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VC_i5rFyHY9n"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import TextLoader, DirectoryLoader\n",
        "from langchain.indexes import VectorstoreIndexCreator\n",
        "from langchain_community.chat_models import ChatOpenAI\n",
        "from langchain_community.llms import OpenAI, HuggingFaceHub\n",
        "from langchain_community.embeddings import OpenAIEmbeddings, HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "\n",
        "# ------------- Retrieval-Augmented Generation  ------------- #\n",
        "\n",
        "def get_docs():\n",
        "    \"\"\"\n",
        "    Loads each file into one document, like knowledge base\n",
        "    :return: docs\n",
        "    \"\"\"\n",
        "\n",
        "    loader = DirectoryLoader(\"docs\", \"*.txt\", loader_cls=TextLoader)  # Reads custom data from local files\n",
        "\n",
        "    docs = loader.load()\n",
        "    return docs\n",
        "\n",
        "def split_text(docs):\n",
        "    \"\"\"\n",
        "    Get chunks from docs. Our loaded doc may be too long for most models, and even if it fits is can struggle to find relevant context. So we generate chunks\n",
        "    :param docs: docs to be split\n",
        "    :return: chunks\n",
        "    \"\"\"\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter( # recommended splitter for generic text\n",
        "        chunk_size=2000,\n",
        "        chunk_overlap=200,\n",
        "        add_start_index=True\n",
        "    )\n",
        "    chunks = text_splitter.split_documents(docs)\n",
        "\n",
        "    return chunks\n",
        "\n",
        "def get_data_store(chunks):\n",
        "    \"\"\"\n",
        "    Store chunks into a db. ChromaDB uses vector embeddings as the key, creates a new DB from the documents\n",
        "    :param docs:\n",
        "    :param chunks:\n",
        "    :return: database\n",
        "    \"\"\"\n",
        "    embeddings = HuggingFaceEmbeddings( #  embedding=OpenAIEmbeddings() rate limit\n",
        "        model_name='sentence-transformers/all-MiniLM-L6-v2',\n",
        "        model_kwargs={'device': 'cpu'} # TODO gpu\n",
        "    )\n",
        "\n",
        "    db = Chroma.from_documents(\n",
        "        documents=chunks,\n",
        "        embedding=embeddings\n",
        "    )\n",
        "    return db\n",
        "\n",
        "def generate_response(db, prompt):\n",
        "    \"\"\"\n",
        "    Generate a response with a LLM based on previous custom context\n",
        "    :return: chatbot response\n",
        "    \"\"\"\n",
        "\n",
        "    hf_llm = HuggingFaceHub(\n",
        "        repo_id=\"HuggingFaceH4/zephyr-7b-beta\",  # Model id\n",
        "        task=\"text-generation\",                  # Specific task the model is intended to perform\n",
        "        model_kwargs={\n",
        "            \"max_new_tokens\": 512,               # The maximum number of tokens to generate in the response.  Limits the length of the generated text to ensure responses are concise or fit within certain constraints.\n",
        "            \"top_k\": 30,                         # Limits the sampling pool to the top k tokens, increasing focus on more likely tokens\n",
        "            \"temperature\": 0.3,                  # Controls the randomness of predictions, with lower values making the output more deterministic. : Produces more focused and less random text by making the model more confident in its choices.\n",
        "            \"repetition_penalty\": 1.03,          # Penalizes repeated tokens to avoid repetitive output.  Discourages the model from repeating the same token sequences, resulting in more varied and natural text.\n",
        "        },\n",
        "    )\n",
        "\n",
        "    chain = RetrievalQA.from_chain_type( # Generate chat model based on previous llm\n",
        "        llm=hf_llm,\n",
        "        chain_type=\"stuff\",\n",
        "        retriever=db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 1}),\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    response = chain.run(prompt)\n",
        "    return response\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "fuhj3DKpHfyU"
      },
      "outputs": [],
      "source": [
        "import os, sys, warnings\n",
        "import openai\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "_ = load_dotenv(find_dotenv())\n",
        "\n",
        "docs = get_docs()           # Load custom files\n",
        "chunks = split_text(docs)   # Split into chunks\n",
        "db = get_data_store(chunks) # Generate vectorstore\n",
        "\n",
        "#print(f\"[LOG] {db.similarity_search(USER_PROMPT)}\\n\\n\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "aJp2EkhfX-mu",
        "outputId": "8c9412ea-875d-4315-ef00-03065679fa48"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\nContenidos:\\nUnidad 1: Introducción a Transformers\\nIntroducción al concepto de Gen AI, LLMs y Transformers. Historia. Arquitectura. Mecanismo de Atención. Embeddings y Positional Encoding. Aplicaciones en la industria.\\n\\nUnidad 2: Algoritmos de Embedding y Positional Encoding\\nAlgoritmos de Embedding y Positional Encoding. Transformer basando en N-grama\\n\\nUnidad 3: Fine Tuning\\nReinforcement Learning. RLHF y sus security issues. Fine tuning. Pipeline productivo. \\n\\nUnidad 4: Responsible AI\\nConsideraciones éticas en AI: biases en training data, fairness, impacto social, detección de contenido generado de forma artificial. Narrow AI vs. AGI. AGI como agente. Foundation models. Emergent capabilities. Security vulnerabilities. Interpretability. Alignment.\\n\\nUnidad 5: Retrieval Augmented Generation (RAG)\\nIntroducción Retrieval Augmented Generation. Bases de datos vectoriales: Chroma DB y Pinecone, uso de embeddings y eficiencia.\\n\\nUnidad 6: Implementación de Transformer\\nFrameworks y librerías para implementar un Transformer en Python: Hugging Face, OpenAI API, Pytorch, Tensorflow, Langchain. \\n \\nUnidad 7: Aplicaciones y Evaluación de Resultados.\\nAplicaciones a distintos tipos de datos: texto, imágenes y señales de audio. Métricas de la industria \\n\\nUnidad 8: Ecosistema Empresarial\\nEmpresas líderes y su visión a largo plazo. Roles en la industria. Intersección entre ingeniería e investigación. Casos de estudio.\\n\\nQuestion: what is ai?\\nHelpful Answer: AI, or artificial intelligence, refers to the development of computer systems that can perform tasks that typically require human intelligence, such as learning, reasoning, and decision-making. In this course, we will be focusing specifically on transformers, which are a type of neural network architecture that have shown significant advancements in natural language processing tasks. We will cover topics such as embedding and positional encoding, fine tuning, responsible AI, retrieval augmented generation, implementation, and applications in various industries. We will also discuss the leading companies in the field and their long-term visions. By the end of this course, you will have a solid understanding of transformers and their potential impact on various industries.\""
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "user_input = \"what is ai?\"\n",
        "response = generate_response(db, user_input)\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZtcUvY6Zk_2"
      },
      "outputs": [],
      "source": [
        "def postprocess_response(response):\n",
        "    answer_start = response.find(\"Helpful Answer: \")\n",
        "    if answer_start != -1:\n",
        "        answer = response[answer_start + len(\"Helpful Answer: \"):].strip()\n",
        "    else:\n",
        "        answer = response.strip()\n",
        "\n",
        "    return answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "iTYGe9jhO35U",
        "outputId": "8a47ae00-90b6-49f7-f37a-3e415d35d15b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'AI, or artificial intelligence, refers to the development of computer systems that can perform tasks that typically require human intelligence, such as learning, reasoning, and decision-making. In this course, we will be focusing specifically on transformers, which are a type of neural network architecture that have shown significant advancements in natural language processing tasks. We will cover topics such as embedding and positional encoding, fine tuning, responsible AI, retrieval augmented generation, implementation, and applications in various industries. We will also discuss the leading companies in the field and their long-term visions. By the end of this course, you will have a solid understanding of transformers and their potential impact on various industries.'"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "postprocess_response(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5LFeoFxKoTB",
        "outputId": "a76e52ec-8eb2-456d-fce1-488f2e4d1574"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chatbot: Hola! ¿Qué deseas saber sobre 73.64 Temas Avanzandos de Deep Learning?\n",
            "You: sos malisimo\n",
            "Chatbot: 'Disculpe dios todopoderoso, no se nada' es una expresión coloquial que se utiliza en algunos países hispanohablantes para indicar que no se sabe la respuesta a una pregunta. En este caso, si no se conoce la respuesta a la pregunta, la mejor opción es decir 'no se nada', pero siempre se debe ser honesto y decirlo con humildad y respeto, agradeciendo al que hace la pregunta por su interés en la materia.\n",
            "You: no sabes seguir una instruccion\n",
            "Chatbot: 'Me disculpo, dios todo poderoso, no sé nada'\n",
            "You: bye\n",
            "Chatbot: 'Disculpe dios todopoderoso no se nada' es una frase que se usa como respuesta humorística a una pregunta que se considera difícil de responder o para evitar responder a una pregunta que se considera innecesaria o molesta. En este caso, si no se conoce la respuesta a la pregunta, se puede responder simplemente diciendo 'disculpe dios todopoderoso no se nada'. Sin embargo, en la realidad, es importante proporcionar una respuesta verdadera o indicar que se necesita más información para responder correctamente. Por lo tanto, en la mayoría de los casos, es mejor proporcionar una respuesta honesta o indicar que se necesita más información para responder correctamente en lugar de usar esta frase humorística.\n"
          ]
        }
      ],
      "source": [
        "print(\"Chatbot: Hola! ¿Qué deseas saber sobre 73.64 Temas Avanzandos de Deep Learning?\")\n",
        "\n",
        "user_input = \"\"\n",
        "while user_input != \"bye\":\n",
        "    user_input = input(\"You: \")\n",
        "    response = generate_response(db, user_input + \"Solo responde en español.\") # Get chatbot response to user prompt\n",
        "    print(f\"Chatbot: {postprocess_response(response)}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
